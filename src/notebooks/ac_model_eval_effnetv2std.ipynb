{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccba0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/mnt/raid/C1_ML_Analysis/source/us-famli-pl/src/\")\n",
    "from nets.classification import EffnetV2sTD, EffnetV2sTDOEMD\n",
    "from loaders.ultrasound_dataset import USAnnotatedBlindSweepDataModule\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _frame_to_png_bytes(frame2d: np.ndarray) -> bytes:\n",
    "    \"\"\"Convert a (C,H,W) frame (any numeric dtype) to PNG bytes for ipywidgets.Image.\"\"\"\n",
    "    f = np.asarray(frame2d)\n",
    "    # Handle NaNs/infs safely\n",
    "    f = np.nan_to_num(f, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # Normalize to uint8\n",
    "    fmin = float(f.min())\n",
    "    fmax = float(f.max())\n",
    "    if fmax > fmin:\n",
    "        u8 = ((f - fmin) / (fmax - fmin) * 255.0).astype(np.uint8)\n",
    "    else:\n",
    "        u8 = np.zeros_like(f, dtype=np.uint8)\n",
    "\n",
    "    img = Image.fromarray(u8, mode=\"RGB\")  # RGB\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, format=\"PNG\")\n",
    "    return buf.getvalue()\n",
    "\n",
    "\n",
    "def visualize_sequence(\n",
    "    images: np.ndarray,\n",
    "    scores: np.ndarray,\n",
    "    scores_pred: np.ndarray,\n",
    "    *,\n",
    "    fps: float = 10.0,\n",
    "    title: str = \"Sequence\",\n",
    "    show_filename: bool = False,\n",
    "    filenames=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Jupyter-stable interactive viewer:\n",
    "      - Left: grayscale frame shown via ipywidgets.Image\n",
    "      - Right: Plotly line plots for GT + Pred with moving markers + vertical line\n",
    "      - Controls: Play + Slider\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images : np.ndarray\n",
    "        Shape (T,H,W) or (C,T,H,W) or (T,H,W,C) supported if you tweak below.\n",
    "        (Your code uses a helper _frame_to_png_bytes(frame) that expects a single frame.)\n",
    "    scores : array-like\n",
    "        Shape (T,), GT score per frame.\n",
    "    scores_pred : array-like\n",
    "        Shape (T,), predicted score per frame.\n",
    "    \"\"\"\n",
    "    images = np.asarray(images)\n",
    "    if images.ndim != 4:\n",
    "        raise ValueError(f\"`images` must have shape (T,H,W,C) (per your current code). Got {images.shape}\")\n",
    "\n",
    "    # Your original code says (C,T,H,W) but then unpacks as (T,H,W,C).\n",
    "    # Keeping your current behavior: images.shape == (T,H,W,C)\n",
    "    T, H, W, C = images.shape\n",
    "\n",
    "    scores_np = np.asarray(scores).reshape(-1)\n",
    "    if scores_np.shape[0] != T:\n",
    "        raise ValueError(f\"`scores` length must match T={T}. Got {scores_np.shape[0]}\")\n",
    "\n",
    "    scores_pred_np = np.asarray(scores_pred).reshape(-1)\n",
    "    if scores_pred_np.shape[0] != T:\n",
    "        raise ValueError(f\"`scores_pred` length must match T={T}. Got {scores_pred_np.shape[0]}\")\n",
    "\n",
    "    if show_filename:\n",
    "        if filenames is None:\n",
    "            raise ValueError(\"show_filename=True requires `filenames=`\")\n",
    "        filenames = np.asarray(filenames).reshape(-1)\n",
    "        if filenames.shape[0] != T:\n",
    "            raise ValueError(f\"`filenames` length must match T={T}. Got {filenames.shape[0]}\")\n",
    "\n",
    "    # --- Widgets ---\n",
    "    slider = widgets.IntSlider(value=0, min=0, max=T - 1, step=1, description=\"Frame\", continuous_update=True)\n",
    "    play = widgets.Play(\n",
    "        value=0, min=0, max=T - 1, step=1,\n",
    "        interval=int(1000 / max(fps, 1e-6)),\n",
    "        description=\"Play\",\n",
    "    )\n",
    "    widgets.jslink((play, \"value\"), (slider, \"value\"))\n",
    "\n",
    "    # Image display (PNG bytes)\n",
    "    img_w = widgets.Image(value=_frame_to_png_bytes(images[0]), format=\"png\")\n",
    "\n",
    "    # Metadata panel\n",
    "    meta = widgets.HTML()\n",
    "    def _meta_html(i: int) -> str:\n",
    "        base = (\n",
    "            f\"<b>frame:</b> {i}\"\n",
    "            f\"<br><b>score (gt):</b> {scores_np[i]:.3f}\"\n",
    "            f\"<br><b>score (pred):</b> {scores_pred_np[i]:.3f}\"\n",
    "        )\n",
    "        if show_filename:\n",
    "            base += f\"<br><b>file:</b> {filenames[i]}\"\n",
    "        return base\n",
    "\n",
    "    meta.value = _meta_html(0)\n",
    "\n",
    "    # Plotly FigureWidget\n",
    "    x = np.arange(T)\n",
    "    fig = go.FigureWidget()\n",
    "\n",
    "    # Lines\n",
    "    fig.add_scatter(x=x, y=scores_np,      mode=\"lines\", name=\"score (gt)\")\n",
    "    fig.add_scatter(x=x, y=scores_pred_np, mode=\"lines\", name=\"score (pred)\")\n",
    "\n",
    "    # Current markers (one for each series)\n",
    "    fig.add_scatter(x=[0], y=[float(scores_np[0])],      mode=\"markers\", name=\"current (gt)\")\n",
    "    fig.add_scatter(x=[0], y=[float(scores_pred_np[0])], mode=\"markers\", name=\"current (pred)\")\n",
    "\n",
    "    # Add max predicted score line\n",
    "    max_pred_idx = int(np.argmax(scores_pred_np))\n",
    "    fig.add_scatter(\n",
    "        x=[max_pred_idx],\n",
    "        y=[float(scores_pred_np[max_pred_idx])],\n",
    "        mode=\"markers\",\n",
    "        name=\"max score (pred)\",\n",
    "        marker=dict(color=\"green\", size=10, symbol=\"x\"),\n",
    "    )\n",
    "\n",
    "    # y-range for the vertical line: cover both series\n",
    "    y_min = float(np.min([scores_np.min(), scores_pred_np.min()]))\n",
    "    y_max = float(np.max([scores_np.max(), scores_pred_np.max()]))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Frame\",\n",
    "        yaxis_title=\"Score\",\n",
    "        margin=dict(l=40, r=10, t=40, b=40),\n",
    "        shapes=[\n",
    "            dict(\n",
    "                type=\"line\",\n",
    "                x0=0, x1=0,\n",
    "                y0=y_min, y1=y_max,\n",
    "                xref=\"x\", yref=\"y\",\n",
    "                line=dict(width=2, dash=\"dash\"),\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    def _update(i: int):\n",
    "        # Update image\n",
    "        img_w.value = _frame_to_png_bytes(images[i])\n",
    "\n",
    "        # Update markers + vertical line\n",
    "        with fig.batch_update():\n",
    "            # marker traces are indices 2 and 3\n",
    "            fig.data[2].x = (i,)\n",
    "            fig.data[2].y = (float(scores_np[i]),)\n",
    "\n",
    "            fig.data[3].x = (i,)\n",
    "            fig.data[3].y = (float(scores_pred_np[i]),)\n",
    "\n",
    "            # keep line spanning both series ranges (or recompute if you want dynamic)\n",
    "            fig.layout.shapes[0].update(x0=i, x1=i, y0=y_min, y1=y_max)\n",
    "\n",
    "        # Update metadata\n",
    "        meta.value = _meta_html(i)\n",
    "\n",
    "    slider.observe(lambda ch: _update(ch[\"new\"]), names=\"value\")\n",
    "\n",
    "    controls = widgets.HBox([play, slider])\n",
    "    left = widgets.VBox([meta, img_w])\n",
    "    right = widgets.VBox([fig])\n",
    "    ui = widgets.VBox([controls, widgets.HBox([left, right])])\n",
    "\n",
    "    return ui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b04996",
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_point = '/mnt/raid/C1_ML_Analysis'\n",
    "\n",
    "dev_id = 0\n",
    "device = torch.device(f'cuda:{dev_id}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model_fn = os.path.join(mount_point, 'train_output/classification/EffnetV2sTD/', 'v0.1/epoch=20-val_loss=0.832.ckpt')\n",
    "# model = EffnetV2sTD.load_from_checkpoint(model_fn, map_location=device)\n",
    "\n",
    "\n",
    "model_fn = os.path.join(mount_point, 'train_output/classification/EffnetV2sTDOEMD/', 'v0.1/epoch=29-val_select=0.227.ckpt')\n",
    "model = EffnetV2sTDOEMD.load_from_checkpoint(model_fn, map_location=device)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a48083",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa6d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = USAnnotatedBlindSweepDataModule(**model.hparams)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53916675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(dm, model):\n",
    "\n",
    "    dirname = os.path.dirname(model_fn)\n",
    "    output_csv = os.path.join(dirname, 'predictions.csv')\n",
    "\n",
    "    if os.path.exists(output_csv):\n",
    "        print(f\"Predictions file {output_csv} already exists. Loading it.\")\n",
    "        df = pd.read_csv(output_csv)\n",
    "        return df\n",
    "    \n",
    "    test_dl = dm.test_dataloader()\n",
    "    test_ds = dm.test_ds\n",
    "\n",
    "    df_scores = []\n",
    "    with torch.no_grad():\n",
    "        for idx, X_d in tqdm(enumerate(test_dl), total=len(test_dl)):\n",
    "            file_path = test_ds.df.iloc[idx]['file_path']\n",
    "            logits = model(X_d['img'].permute(0, 2, 1, 3, 4).to(device))  # [B, N, C]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            levels = torch.tensor([0.0, 0.25, 0.5, 0.75, 1.0], device=probs.device)\n",
    "            score = (probs * levels).sum(dim=-1)  # [B, N]\n",
    "\n",
    "            df = pd.DataFrame({\n",
    "                'frame_index': list(np.arange(X_d['img'].shape[2])),\n",
    "                'file_path': file_path,\n",
    "                'score_pred': score.squeeze().cpu().numpy(),\n",
    "                'score': X_d['scalar'].squeeze().cpu().numpy(),\n",
    "            })\n",
    "            df_scores.append(df)    \n",
    "    df = pd.concat(df_scores, ignore_index=True)\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved predictions to {output_csv}\")\n",
    "    return df\n",
    "\n",
    "df_test_pred = run_test(dm, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b251014",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pred['error'] = df_test_pred['score_pred'] - df_test_pred['score']\n",
    "df_test_pred['abs_error'] = df_test_pred['error'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pred['abs_error'].describe(percentiles=[0.5, 0.75, 0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.array([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "\n",
    "def to_class(x):\n",
    "    return np.argmin(np.abs(bins - x))\n",
    "\n",
    "df_test_pred[\"y_true\"] = df_test_pred.score.apply(to_class)\n",
    "df_test_pred[\"y_pred\"] = df_test_pred.score_pred.apply(to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77868242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "cm = confusion_matrix(df_test_pred['y_true'], df_test_pred['y_pred'])\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(bins))\n",
    "\n",
    "fmt = '.3f' \n",
    "thresh = cm_norm.max() / 2.\n",
    "for i, j in itertools.product(range(cm_norm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.xticks(tick_marks, [f\"{b:.2f}\" for b in bins])\n",
    "plt.yticks(tick_marks, [f\"{b:.2f}\" for b in bins])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf9219",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(df_test_pred['y_true'], df_test_pred['y_pred'], target_names=['0.0', '0.25', '0.5', '0.75', '1.0'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09366e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = df_test_pred.score == 1.0\n",
    "plt.hist(df_test_pred.score_pred[m], bins=30)\n",
    "plt.axvline(1.0, color=\"r\")\n",
    "plt.title(\"Predictions for true high_measurable (1.0)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e72a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [0.7, 0.8, 0.9]:\n",
    "    tp = np.mean(df_test_pred.score_pred[df_test_pred.score == 1.0] >= t)\n",
    "    print(f\"P(score_pred >= {t} | true=1.0) = {tp:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd15ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [0.5, 0.9]:\n",
    "    tp = np.mean(df_test_pred.score_pred[df_test_pred.score == 0.0] >= t)\n",
    "    print(f\"P(score_pred >= {t} | true=0.0) = {tp:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace14510",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seq = (\n",
    "    df_test_pred.groupby(\"file_path\")\n",
    "      .agg(score_max=(\"score\",\"max\"), pred_max=(\"score_pred\",\"max\"))\n",
    "      .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f47be4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.violin(df_seq, y=\"pred_max\", x=\"score_max\", box=True, points=\"all\", title=f\"Predicted max score vs GT max score per sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.violin(df_test_pred, y=\"score_pred\", x=\"score\", box=True, points=\"outliers\", title=f\"Predicted score vs GT score per frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f8d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in [0, 0.25, 0.5, 0.75, 1.0]:\n",
    "    print(f\"Score: {s}\")\n",
    "    print(df_test_pred.query(f'score == {s}')['score_pred'].describe(percentiles=[.01, .1, .25, 0.5, 0.75, 0.9, 0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b88420",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = dm.test_ds\n",
    "df_frames = test_ds.df_frames\n",
    "df = test_ds.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ce87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_seq.query('score_max == 0.25 and pred_max > 0.8')['file_path'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2 \n",
    "file_paths = list(df_seq.query('score_max <= 0.25 and pred_max > 0.7')['file_path'].drop_duplicates())\n",
    "print(len(file_paths))\n",
    "file_path = file_paths[idx]\n",
    "\n",
    "# idx = 0\n",
    "# file_paths = list(df_seq.query('score_max >= 0.75 and pred_max <= 0.25')['file_path'].drop_duplicates())\n",
    "# print(file_paths)\n",
    "# file_path = file_paths[idx]\n",
    "\n",
    "print(file_path)\n",
    "q = df_test_pred.query('file_path == @file_path')\n",
    "score, score_pred = q['score'], q['score_pred']\n",
    "\n",
    "img = sitk.ReadImage(os.path.join(mount_point, file_path))\n",
    "img_np = sitk.GetArrayFromImage(img)  # [T,H,W]\n",
    "\n",
    "if img.GetNumberOfComponentsPerPixel() == 1:\n",
    "    img_np = np.expand_dims(img_np, -1).repeat(3, axis=-1)\n",
    "\n",
    "visualize_sequence(img_np, score, score_pred, title=f\"{'/'.join(file_path.split('/')[-2:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085552ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e1036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_example():\n",
    "\n",
    "    file_path = df_frames.query('annotation_label == \"high_measurable\" or annotation_label == \"low_measurable\" or annotation_label == \"high_visible\" and tag != \"AC\"')['file_path'].drop_duplicates().sample(n=1).values[0]\n",
    "    \n",
    "    print(file_path)\n",
    "    idx = df.query(f'file_path == \"{file_path}\"').index[0] \n",
    "    X_d = test_ds[idx]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_d['img'].unsqueeze(0).permute(0, 2, 1, 3, 4).to(device))\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        levels = torch.tensor([0.0, 0.25, 0.5, 0.75, 1.0], device=probs.device)\n",
    "        score = (probs * levels).sum(dim=-1)  # [B, N]\n",
    "\n",
    "\n",
    "    visualize_sequence(X_d['img'].permute(1,2,3,0), X_d['scalar'], score.squeeze(0).cpu().numpy(), title=f\"{'/'.join(file_path.split('/')[-2:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33603298",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_bins = torch.arange(1, 6, dtype=torch.float32) \n",
    "w_bins = w_bins / w_bins.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ce341",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([369458, 92519, 15579, 7674, 5970])\n",
    "w = 1.0 / c\n",
    "w = w / w.mean()\n",
    "w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38453fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = np.array([0.2,0.4,0.6,1.0,2.0])\n",
    "wb = wb / wb.mean()\n",
    "wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5757a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
